<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="css/multiColumnTemplate.css" rel="stylesheet" type="text/css">
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
<link rel="stylesheet" href="css/cmp.css" />
<title>CMP | Examples</title>
</head>

<body>
<div class="container" id="page-container">
  <div id="content-wrap">
    <header>
      <div class="primary_header">
        <h1 class="title"><a href="index.html" id="title-link">COMPUTER MUSIC PROGRAMMING</a></h1>
      </div>
      <nav class="secondary_header" id="menu">
        <ul>
          <li><a href="topics.html">TOPICS</a></li>
          <li><a href="examples.html">EXAMPLES</a></li>
          <li><a href="resources.html">ADDITIONAL RESOURCES</a></li>
          <li><a href="week-by-week.html">WEEK-BY-WEEK PLAN</a></li>
          <li><a href="about.html">ABOUT</a></li>
        </ul>
      </nav>
    </header>
    <section>
      <div class="container">
        <h1 id="computer-music-programming">List of Examples</h1>
        <p>Go to <a href="#max">"Max Examples"</a> or <a href="#webaudio">"WebAudio Examples"</a>.</p>
        <hr />
        <h2><a name="max">Max Examples</a></h2>
        <p> The largest single collection of Max examples is the <a href="http://music.arts.uci.edu/dobrian/maxcookbook/">Max Cookbook</a>. You can find specific examples there by title or by keyword search. </p>
        <p><strong>[Each image below is linked to a file of JSON code containing the actual Max patch.<br />
          Right-click on an image to download the .maxpat file directly to disk, which you can then open in Max.]</strong> </p>
        <p> <a name="Ex01"><b>Example 1</b></a>: Fade-in and fade-out </p>
        <p> <a href="examples/max/patches/fadeinfadeoutdemo.maxpat"><img src="examples/max/images/fadeinfadeoutdemo.png" width="467" height="484" alt="Example 1" border=0></a> </p>
        <p> This shows the use of linear interpolation from one amplitude to another to create a smooth fade-in or fade-out. The <b>line~</b> object expects to receive a ramp time in milliseconds in its right inlet, and then a destination value in its left inlet. (It can also receive those two numbers as a two-item list, &lt;destination value&gt;&lt;ramp time&gt;, in its left inlet.) Its output signal will then change linearly from its current value to its new destination value in the specified amount of time. Once it arrives at the destination value, its output signal stays constant at the new value. </p>
        <p> Because the fade-out takes a non-zero amount of time, it's necessary to delay stopping the sound file until the fade-out has taken place. So, whereas we can start the fade-in at the same time as we start the sound file, we need to wait for the fade-out to finish before we stop the file. So, we look for the off message (0), using a <b>select</b> object, which will send out a <i>bang</i> when it receives a match, and we then delay that bang with a <b>delay</b> object until the fade-out is done, then the <i>bang</i> triggers the message <i>0</i> to stop the <b>sfplay~</b> object. </p>
        <p> <a name="Ex02"><b>Example 2</b></a>: Vibrato by means of frequency modulation </p>
        <p> <a href="examples/max/patches/modulationdemo.maxpat"><img src="examples/max/images/modulationdemo.png" width=899 height=483 alt="Example 2" border=0></a> </p>
        <p> The <b>cycle~</b> object is an oscillator that produces a sinusoidal tone at any specified frequency. At an audio frequency we can listen to it, and at a sub-audio frequency we can use it as a low-frequency oscillator (LFO) to modulate some parameter of a sound. This patch demonstrates frequency modulation to create a vibrato effect. Try changing the different number boxes until you have a good experience of how each parameter affects the sound, especially the rate and depth of the frequency modulation caused by the modulating oscillator. Try some extreme values to get extreme effects. </p>
        <p> Because the fade-out takes a non-zero amount of time, it's necessary to delay stopping the sound file until the fade-out has taken place. So, whereas we can start the fade-in at the same time as we start the sound file, we need to wait for the fade-out to finish before we stop the file. So, we look for the off message (<i>0</i>), using a <b>select</b> object, which will send out a <i>bang</i> when it receives a match, and we then delay that bang with a <b>delay</b> object until the fade-out is done, then the <i>bang</i> triggers the message <i>0</i> to stop the <b>sfplay~</b> object. </p>
        <p> <a name="Ex03"><b>Example 3</b></a>: MIDI Keyboard Example </p>
        <p> <a href="examples/max/patches/midikeyboard.maxpat"><img src="examples/max/images/midikeyboard.png" width=892 height=579 alt="Example 3" border=0></a> </p>
        <p> <a name="Ex04"><b>Example 4</b></a>: Mouse Theremin </p>
        <p> <a href="examples/max/patches/mousetheremin.maxpat"><img src="examples/max/images/mousetheremin.png" width=1041 height=900 alt="Example 4" border=0></a> </p>
        <p> <a name="Ex05"><b>Example 5</b></a>: Saw Synth Demo </p>
        <p> <a href="examples/max/patches/sawsynthdemo.maxpat"><img src="examples/max/images/sawsynthdemo.png" width=1306 height=479 alt="Example 5" border=0></a> </p>
        <p> <a name="Ex06"><b>Example 6</b></a>: Schedule a future event </p>
        <p> <a href="examples/max/patches/schedulefutureevent.maxpat"><img src="examples/max/images/schedulefutureevent.png" width=544 height=306 alt="Example 6" border=0></a> </p>
        <p> Timing is very important in music. The fundamental way to ensure precise timing of events is to use a scheduler. A schedule is a list of time-tagged events to be executed at specific times in the future. That schedule must be consulted constantly at regular intervals (as often as possible, e.g., every millisecond) to see if any item on the list has a time tag that is less than or equal to the current time; if so, that event should be enacted. </p>
        <p> Max has a master scheduler constantly running behind the scenes, and has various objects that allow you to post events on the schedule. The simplest of all of those is the <b>delay</b> object. When <b>delay</b> receives a <i>bang</i> in its left inlet, it schedules that <i>bang</i> to be sent out its outlet after a delay of a certain number of milliseconds, which has been specified in its right inlet. (Alternatively, if <b>delay</b> receives an <i>int</i> or a <i>float</i> in its left inlet, it uses that number to set its delay time, overwriting whatever number was previously set as the delay time, and schedules a <i>bang</i> to be sent out after that number of milliseconds has elapsed.) </p>
        <p> The <b>delay</b> object can schedule only one <i>bang</i> at a time. If another <i>bang</i>, <i>int</i>, or <i>float</i> is received in its left inlet before the previously-scheduled <i>bang</i> has been sent out, the previously-scheduled <i>bang</i> will be pre-empted and the new <i>bang</i> will be scheduled instead. The message <i>stop</i> in the left inlet cancels any currently-scheduled <i>bang</i>. If the delay time in the right inlet is changed while a <i>bang</i> is scheduled, that doesn't change the timing of the existing scheduled <i>bang</i>; the new delay time only affects subsequently-scheduled <i>bang</i>s. </p>
        <p> In this example, we use a bass drum sound and a snare drum sound as two stand-ins for the idea of "an event", but in fact an event can be any Max message, or combination of messages, even turning on or off other entire processes. The delay time might be only a few milliseconds, to make two things happen in rapid precisely-timed succession, or it could be a much longer time (though still expressed in terms of milliseconds) such as several seconds, minutes, or even hours. </p>
        <p> Although it's not demonstrated in this example, it's also possible to specify the delay time in <a href="https://docs.cycling74.com/max7/vignettes/maxtime_syntax">tempo-relative time units</a>, such as <i>4n</i> for a quarter note, based on the current tempo of the Max <a href="https://docs.cycling74.com/max7/maxobject/transport">transport</a>, but in that case <b>delay</b> will only work when the transport is on. </p>
        <p> The Max <b>delay</b> object is comparable to the <code>setTimeout()</code> method in JavaScript. When using JavaScript within a Max <b>js</b> object, the <code>setTimeout()</code> method is not available; the nearest equivalent is the <code>schedule()</code> method of <a href="https://docs.cycling74.com/max7/vignettes/jstaskobject">the Task() object</a>, but that will run in a low-priority thread of Max, not in the high-priority scheduler as Max's <b>delay</b> object does. </p>
        <p> <a name="Ex07"><b>Example 7</b></a>: DIY ring buffer </p>
        <p> <a href="examples/max/patches/ringbufferdemo.maxpat"><img src="examples/max/images/ringbufferdemo.png" width=741 height=545 alt="Example 7" border=0></a> </p>
        <p> For time-delayed audio, we need to create what's called a "<a href="https://en.wikipedia.org/wiki/Circular_buffer">circular buffer</a>" or a "ring buffer", an array of samples to which we record continuously in a loop, and which we can then use always to access the most recently recorded sound from the past. </p>
        <p> The MSP object <b>delay~</b> does exactly that. And another pair of objects&mdash;<b>tapin~</b> and <b>tapout~</b> does the same thing, but with a minimum delay of one signal vector so that you can even feed the delayed sound back into the delay loop. However, in this example, we build and access a ring buffer ourselves, just to demonstrate inner workings of the process in a little more detail. </p>
        <p> The <b>buffer~</b> object creates an array in RAM for storing sound and accessing it in a variety of ways. Here we create a one-second buffer, and we use a sample counter (the <b>count~</b> object) to put each sample of the incoming sound (from <b>adc~</b>) into that array, using the <b>poke~</b> object to access individual sample locations in the array. At the same time, we can use that same sample counter, which is always pointing to the location of the current time in the array, to calculate the signal some amount of time (some number of samples) in the past. If the current time in samples minus the delay time in samples is less than 0, we need to wrap the calculated sample location back into the array (by adding the number of samples in the array); that's what the <b>pong~</b> object does for us. </p>
        <p> You can try this out yourself, but beware of the possibility of feedback, since this patch sends the incoming sound from <b>adc~</b>, with some delay, right back out via the <b>dac~</b>. </p>
        <p> <a name="Ex08"><b>Example 8</b></a>: Algorithmic demo with phasor </p>
        <p> <a href="examples/max/patches/algorithmicdemowithphasor.maxpat"><img src="examples/max/images/algorithmicdemowithphasor.png" width=536 height=400 alt="Example 8" border=0></a> </p>
        <p> <a name="Ex09"><b>Example 9</b></a>: Amplitude is inversely proportional to distance </p>
        <p> <a href="examples/max/patches/distance&amplitude.maxpat"><img src="examples/max/images/distance&amplitude.png" width=412 height=346 alt="Example 9" border=0></a> </p>
        <p> Our tympanic membrane (a.k.a. our eardrum) and a microphone are both devices that measure sound intensity. When a sound arrives at our eardrum or at the diaphragm of a microphone, either of which has a certain surface area, the power in that area (i.e. the intensity) is detected. However, the intensity of a sound, as measured by an eardrum or a microphone, will differ depending on the distance from the sound's source, because the sound is being emitted from the source in <u>all</u> directions. If you think of the sound energy as radiating outward from the source in a spherical pattern, and you bear in mind that the surface of a sphere is proportional to the square of its radius (the surface area of a sphere is equal to <em>4πr<sup>2</sup></em>), you can understand that the intensity of a sound as measured in a given surface area is inversely proportional to the square of the distance of the point of measurement from the sound source. This principle is known as the <em><a href="http://hyperphysics.phy-astr.gsu.edu/hbase/Forces/isq.html">inverse square law</a></em>: intensity is inversely proportional to the square of the distance from the source (<em>I &prop; 1/d<sup>2</sup></em>). </p>
        <p> Our subjective sense of a sound's "loudness" is not the same as its intensity, but is generally roughly proportional to it. But what does that mean in terms of the <u>amplitude</u> factor we'll use to alter a sound's intensity in digital audio? As defined in physics, the intensity of a wave is proportional to the square of its amplitude (<em>A<sup>2</sup> &prop; I</em>). So that means that if we want to emulate the effect of a sound being twice as far away, (1/4 the intensity), we would need to multiply the amplitude by one-half. Indeed, based on what we know about the relationship between distance and intensity (the inverse square law, <em>I &prop; 1/d<sup>2</sup></em>), we can see that the relationship between distance and amplitude is simply <em>A ∝ 1/d</em>; amplitude is inversely proportional to distance. </p>
        <p> This patch shows how you can emulate a change of distance simply by changing its relative amplitude inversely. Of course, our sense of distance is also affected by reverberation and high-frequency rolloff, but this basic relationship between distance and ampitude is useful to know for sound spatialization. </p>
        <p> <a name="Ex10"><b>Example 10</b></a>: scope~ and spectroscope~ in Max </p>
        <p> <a href="examples/max/patches/scope~_and_spectroscope~_tutorial.maxpat"><img src="examples/max/images/scope~_and_spectroscope~_tutorial.png" alt="Example 10" border=0></a> </p>
        <!--
      <div id="video-script1" class="row">
        <div id="video" class="col">
          <video controls
                 src="examples/max/scope~_and_spectroscope~_tutorial.mp4"
                 width="100%"
                 height="100%"
                 >
        </div>
        <div id="script1" class="col">
          <button id="toggle-script" type="button" class="btn btn-primary">Show/Hide Text</button>
          <div id="script-text" style="display: none; overflow-y: scroll">

            <h3>scope~ and spectroscope~ in Max</h3>

            <p>
              This is a short introduction to visualizing signals in Max. When we want to produce a sine wave, we create a cycle~ object. In order to listen to this pure tone, we need to connect it to the speaker outputs and enable the DSP processing. We can create an ezdac~ object for this effect and, just as a precaution, we will also create a gain~ object to control the signal level coming out of our speakers. In terms of visual feedback, in Max we have a couple objects that can help us better understand the signal that we are producing. The meter~ object, as the name indicates, is a simple object that signals the level of the incoming signal, this can be useful for example to quickly figure out if one or the other object is sending a weak or strong signal, or no signal at all. In a similar fashion, the number~ object tells the exact amplitude value of the samples coming in. But then, we have the scope~ object which gives us a more detailed visual feedback of the incoming signal. With this object we can clearly see the waveform of the incoming signal and how it compresses when we raise the frequency. There are different ways to adjust this window's resolution, but for the time being let's just set it in automatic mode. If we create an additional cycle~ object, with a different frequency, and add it to our original signal, we can see that it makes the waveform look slightly more complex. And if we add more cycle~ objects, it will get even more complex. Another object worth looking at, when trying to understand our signals is the spectroscope~. This object, instead of showing amplitude over time, will show us amplitude over frequency. In our current example we can see that there are three peaks corresponding to the three sine waves that we added together - it's the waveform complexity put into perspective, if you will. Waveforms other than the sine (or cosine) wave, will have different complexities. The rect~ object creates a square wave that, as you can see, it's a rather complex waveform. In turn, the saw~ object creates a sawtooth looking waveform, as you can confirm by the scope~ object, and it is also a complex waveform with different rules for its partials.
            </p>
          </div>
        </div>
      </div>
      <p>(<a href="examples/max/patches/scope~_and_spectroscope~_tutorial.maxpat">Download the patch</a>)</p>
      -->
        <ul>
          <li><a href="topics/intro-to-max-and-msp/4.scope~_and_spectroscope~_tutorial.html">See lesson on scope~ and spectroscope~ in Max</a></li>
        </ul>
        <p> <a name="Ex11"><b>Example 11</b></a>: Line and Line~ </p>
        <p> <a href="examples/max/patches/line_object_tutorial.maxpat"><img src="examples/max/images/line_object_tutorial.png" alt="Example 11a" border=0></a> </p>
        <p><b>Line object</b></p>
        <p>Often we will find ourselves in the situation where we need to automate the change of a certain value in time, even if only to free ourselves from the burden of having to control the machine. Though there are always multiple ways to achieve the same result, as it happens in most programming languages, in Max we have an object whose sole purpose is to help us solve this specific problem. The line object can interpolate between two numbers in a given time interval. Given a new line object, we can put it to work by sending it a list with 2 numbers. The first number is the number that we want to reach; the second number is the time interval, in milliseconds, for that number to be reached. Using your max skills you can change this message box to include a place holder, so that you don't have retype the message box everytime you need to change its values. Should you send it a single value, the line object will immediatelly jump to that number. Furthermore, you can use this technique to your advantage if you decide to always have the numbers changing in the same fashion, say from 10 to 50 in 1500 milliseconds with the push of a button. Finally, you can also instruct the line object to work with floats, instead of integers, if you set the first argument as an initial floating point number. </p>
        <br>
        <p> <a href="examples/max/patches/line~_object_tutorial.maxpat"><img src="examples/max/images/line~_object_tutorial.png" alt="Example 11b" border=0></a> </p>
        <p><b>Line~ object</b></p>
        <p>As with many other objects in Max, the line object also has its MSP counterpart, the line~ object. This object functions very much like its Max counterpart with the exception that it outputs a signal instead of a stream of messages. Again, by sending it a list with two numbers the line~ object will interpolate between its current value and the first number you are sending, in the time interval of the second number on the list. This second number can also be sent to the second inlet. The line~ object only has two inlets instead of three because the time grain, the rate at which it is updating its internal value (the values that get sent out) is determined by the audio status settings. Having line object that works in the signal domain allow us to directly interact with other MSP objects. Here I'm multiplying the output of the line~ with the output of a cycle~ object. And sure enough, we can send a longer list of values in order to sculpt our signal with the precision we need. In this example, I will have the sound fade in during 2 seconds, fade out during 2 seconds, jump to full volume and stay there for 200 milliseconds, jump to silence and stay there for 200 milliseconds, and finally jump back to full volume and fade out during 2 seconds. </p>
        <!--
      <div class="row">
        <div class="col">
          <video controls
                 src="topics/control-signals/2.line_object_tutorial.mp4"
                 width="100%"
                 height="100%"
                 >
        </div>
        <div class="col">
          <video controls
                 src="topics/control-signals/2.line~_object_tutorial.mp4"
                 width="100%"
                 height="100%"
                 >
        </div>
      </div>
      <p>(<a href="examples/max/patches/line_object_tutorial.maxpat">Download the patch</a>)</p>
      <p>(<a href="examples/max/patches/line~_object_tutorial.maxpat">Download the patch</a>)</p>
      -->
        <ul>
          <li><a href="topics/control-signals/2.line-and-line~.html">See lesson on Line and Line~</a></li>
        </ul>
        <p> <a name="Ex12"><b>Example 12</b></a>: Time-stretch a sound </p>
        <p> <a href="examples/max/patches/timestretchasound.maxpat"><img src="examples/max/images/timestretchasound.png" width="336" height="270" alt="Example 12" border=0></a> </p>
        <p> When you change the playback rate of a sound, its speed and total duration change, and as a result, its pitch changes. Time-stretching a sound with phase vocoding results in a change of speed and duration, without a change of pitch. When you turn on the timestretch attribute of <b>sfplay~</b>, by sending the message <i>timestretch 1</i>, the rate value in the right inlet functions as a time-stretching factor rather than a rate factor, so there's no change in pitch. Try comparing the behavior of <b>sfplay~</b> with <i>timestretch</i> on and <i>timestretch</i> off, and with different time-stretching factors. </p>
        <p> <a name="Ex13"><b>Example 13</b></a>: Pitch-shift a sound </p>
        <p> <a href="examples/max/patches/pitchshiftasound.maxpat"><img src="examples/max/images/pitchshiftasound.png" width="349" height="335" alt="Example 13" border=0></a> </p>
        <p> To pitch-shift a sound using phase vocoding, without any change in its speed or duration, use the <b>pitchshift</b> object. You can specify the amount of pitch shift as a ratio (a frequency factor) or, probably more usefully for musical purposes, you can change the pitch in cents (1/100ths of an equal-tempered musical semitone) with the <i>pitchshiftcent</i> attribute. For example, sending the object the message <i>pitchshiftcent -1200</i> shifts the pitch downward by one octave (equal to 12 semitones). </p>
        <p> <a name="Ex14"><b>Example 14</b></a>: Ring modulation </p>
        <p> <a href="examples/max/patches/ringmod.maxpat"><img src="examples/max/images/ringmod.png" width="535" height="370" alt="Example 14" border=0></a> </p>
        <p> The principles of ring modulation (multiplication of two audio signals) and amplitude modulation are discussed in the MSP Tutorials titled <a href="https://docs.cycling74.com/max8/tutorials/06_synthesischapter02" target="_blank">"Tremolo and Ring Modulation"</a> and <a href="https://docs.cycling74.com/max8/tutorials/06_synthesischapter03" target="_blank">"AM Synthesis"</a>. This patch modulates a sound file by a cosine wave, allowing you to experiment with different modulation rates (from sub-audio low frequencies for tremolo effects to audio-rate frequencies to create frequency shifts up and down), and to try different depths of modulation (by changing the amplitude of the modulating oscillator), and to add an offset to shift the modulator up or down. </p>
        <p> <a name="Ex15"><b>Example 15</b></a>: Select one of four classic waveform types </p>
        <p> <a href="examples/max/patches/waveforms.maxpat"><img src="examples/max/images/waveforms.png" width="895" height="333" alt="Example 15" border=0></a> </p>
        <p> There are four basic "classic" waveforms that were used in early synthesizers: sine, triangle, square, and sawtooth. Max produces anti-aliased emulations of those waveforms with the objects <b>cycle~</b>, <b>tri~</b>, <b>rect~</b>, and <b>saw~</b>. </p>
        <p> Many software synthesizers, such as the Subtractor synthesizer in Reason, allow you to select a stored waveform to use in an oscillator. This patch shows how you can do that in Max with the four classic wave types. </p>
        <p> In MSP, to select one of several possible audio signals, you can use the <b>selector~</b> object. The first argument of <b>selector~</b> determines how many signal inlets it will have, and an additional left inlet is created to specify which inlet's signal should be passed out the object's outlet. In this patch, we have created a <b>selector~</b> with 4 signal inlets, for the four types of oscillator. The <b>umenu</b> object allows you to make a popup menu, and whichever item the user selects is reported out the outlet. The middle outlet reports the name of the menu item, and the left outlet reports the item number, starting from 0. So, the output from the left outlet of the <b>umenu</b> can be used to control the <b>selector~</b>. </p>
        <p> To set up initial conditions in a patch, the objects <b>loadbang</b> and <b>loadmess</b> are essential for triggering messages as soon as the patch is opened. In this patch, the <b>loadmess</b> object is used to send the message <i>startwindow</i> to the <b>dac~</b> object, turning on audio in only this Patcher window and its subpatches, while turning off audio in every other open window. </p>
        <p> Another way to set up the initial value of a user interface object (and the object connected to its outlet) is by setting "Parameter Mode Enable" in the object's Inspector, then setting "Initial Enable" in the Inspector and setting the "Initial Value" to the initial value you want. When the patch is opened, that initial value will be shown in the object, and will be sent out the outlet. That way, the user interface object, and the object it controls, will be set up properly to the desired initial state. In this patch, the <b>umenu</b> is set up initially to item 1, "Sine", which opens inlet 1 (the <b>cycle~</b> object) of the <b>selector~</b>. The <b>live.gain~</b> obejct's initial value is set to <i>-10</i> dB to prevent the output sound being too loud. Initially, all the oscillators have a frequency of 0 Hz, so we don't hear anything until we click on the <b>kslider</b> keyboard to select a pitch (which then gets converted to the appropriate frequency by the <b>mtof</b> object. </p>
        <p> <a name="Ex16"><b>Example 16</b></a>: Detune and mix two oscillators </p>
        <p> <a href="examples/max/patches/pitchoffsets.maxpat"><img src="examples/max/images/pitchoffsets.png" width="1005" height="642" alt="Example 16" border=0></a> </p>
        <p> In the Subtractor synthesizer in Reason, two carrier oscillators are mixed together. Either can be any of a number of different waveforms, and either can be detuned by any number of octaves, semitones, and/or cents (100ths of a semitone). This patch emulates those same capabilities. </p>
        <p> The different components of the patch are encapsulated in <b>patcher</b> objects. The detunings are done by adding offsets of either multiples of 12 semitones (for octaves), or some number of semitones, or 100ths of semitone. Then the resulting pitches are converted to frequency for the oscillators. The two oscillators are mixed in the the <b>patcher <i>mix</i></b> object, the contents of which are explained in the Max Cookbook example titled <a href="https://music.arts.uci.edu/dobrian/maxcookbook/mix-two-signals-more-efficiently" target="_blank">"Mix two sounds (more efficiently)"</a>. </p>
        <p>Experiment with different wave types, detunings, and mixtures.</p>
        <p> <a name="Ex17"><b>Example 17</b></a>: ADSR envelope for 2-oscillator synth </p>
        <p> <a href="examples/max/patches/amplitudeenvelope.maxpat"><img src="examples/max/images/amplitudeenvelope.png" width="837" height="724" alt="Example 17" border=0></a> </p>
        <p> This patch is based on the 2-oscillator synthesizer in the example titled "Detune and mix two oscillators". Here we have added ADSR amplitude envelope control, using the <b>adsr~</b> object, which is explained in the Max Cookbook example <a href="https://music.arts.uci.edu/dobrian/maxcookbook/adsr-amplitude-envelope" target="_blank">"ADSR amplitude envelope"</a>. The keyboard has been set to "Touchscreen" mode in the Inspector, which makes it send out a note-on velocity when you click on a key, and send out a 0 velocity when the cursor leaves the key or the mouse button is released. You can also provide note-on and note-off messages from an attached MIDI keyboard, via the <b>notein</b> object. </p>
        <p> Try different attack and release times to get a sense of how those parameters of the amplitude envelope affect the onset and termination of a note. Notice that the effect of the note-on velocity and the of four dials is not a simple linear mapping into amplitude level or time in milliseconds. The input values to <b>adsr~</b> are each obtained using mathematical formulae that have been chosen to try to make the most useful possible mapping. For example, a note-on velocity of 127 results in a peak amplitude of 1., and each reduction of 16 from there results in a change of -6 dB, corresponding to our perception of loudness. The dials for attack, decay, and release times use an exponential scaling formula that concentrates the first half of the dial in very short time values, less than 100ms, and only the upper fourth of the dial in time values greater than 1 second. The sustain level is shown to the user in the perceptually intuitive units of decibels, and then later converted to linear amplitude for <b>adsr~</b>. </p>
        <p> <a name="Ex18"><b>Example 18</b></a>: Wobble bass with rhythmic filter sweeps </p>
        <p> <a href="examples/max/patches/wobblebass.maxpat"><img src="examples/max/images/wobblebass.png" width="579" height="592" alt="Example 18" border=0></a> </p>
        <p> A common sound in dubstep music is the so-called "wobble" bass, a bass line that implies a repetitive rhythm through some internal characteristic of its sound, usually an LFO-controlled rhythmic sweep of a lowpass or bandpass filter. In this example, we use a sawtooth waveform as the control signal to sweep a bandpass filter up and down on a harmonically-rich sawtooth oscillator. </p>
        <p> The example shows how the MSP <b>phasor~</b> object can have its rate specified as a transport-dependent note value such as <i>8n</i>, <i>8nt</i>, <i>16n</i>, etc., and can be locked to be in sync with the transport's sense of beat timing. Notice that the amplitude of the oscillator is not being changed in this example, only its filtering, but the upward filter sweep is so sudden as to give the impression of an attack on each of the specified beat divisions. The resonant center frequency of the bandpass filter&mdash;in the third inlet of the <b>reson~</b> object&mdash;is always based on the fundamental frequency of the note currently being played by the oscillator. The control signal&mdash;from the <b>phasor~</b> object&mdash;offsets the filter frequency suddenly up to 8 times that (i.e., three octaves above the fundamental), then ramps it back down to the fundamental. </p>
        <p> Turn audio on, click on the <b>toggle</b> to turn on the <b>transport</b>, then choose different note values to change the rhythmic rate of the <b>phasor~</b>. </p>
        <p> <a name="Ex19"><b>Example 19</b></a>: Convolution Reverb </p>
        <p> <a href="examples/max/patches/convolution.zip"><img src="examples/max/images/Convolution.png" width="857" height="765" alt="Example 19" border=0></a> </p>
        <p> <a href="examples/max/patches/convolution.zip"><img src="examples/max/images/Convolution_fft.png" width="800" height="309" alt="Example 19 FFT" border=0></a> </p>
        <p> A good way to add realistic reverb to a sound is convolution. In essence, convolution is the multiplication of two signals in the time domain. To create reverb using this method, every sample of the dry sound is multiplied with every sample of an impulse response or IR in short. An IR is a representation of what will be heard when a very short pulse of white noise is played in a certain space. A close approximation of how another dry sound might be heard in that space can be achieved by convolving that sound with the IR. </p>
        <p> An efficient way of performing convolution is to do the calculations in the frequency domain rather than the time domain. In the frequency domain, convolution is the multiplication of the magnitude and the addition of the phase of each frequency bin. In this example, an IR with 6144 samples is used. The patch shown in the bottom image performs the convolution, and three instances of this patch are loaded inside the <b>pfft~</b> objects in the main patch shown in the top image. The <b>pfft~</b> instances are performing convolution using different segments of the IR. The dry signal is fed to each instance with a delay time corresponding to the segment of the IR that the instance is dealing with. This process creates convolution reverb with a tail length of 6144 samples. </p>
        <p> N.B.: Once you expand the zip file for this example, you will find three files. For this example to work, they have to be in the same folder. <i>Convolution_Long.maxpat</i> is the main patch, <i>Con_FFT.maxpat</i> is the subpatch used by the <b>pfft~</b> objects, and <i>Impulse_Long.aiff</i> is the impulse response. </p>

        <hr>
        <hr />
        <h2><a name="webaudio">WebAudio Examples</a></h2>
        <p> <a name="Ex01js"><b>Example 1</b></a>: Synthesizer with vibrato </p>
        <p>Here is a basic synthesizer in Web Audio, designed by Chris, with a sawtooth oscillator, a simple attack-release amplitude envelope, and sinusoidal vibrato by means of frequency modulation with an LFO. The notes are played by keys of the computer keyboard. MIDI velocity is simulated by setting a note-on velocity value to control the notes' peak amplitude. The rate and depth of the vibrato can also be set by the user. </p>
        <p> <a href="examples/javascript/vibratosynth.html">Synthesizer with vibrato &lt;vibratosynth.html&gt;</a> </p>
        <p> There are lots of explanatory comments in the <a href="examples/javascript/vibratosynth.js">vibratosynth.js</a> file. </p>
        <p> <a name="Ex02js"><b>Example 2</b></a>: Oscilloscope and Spectroscope in Webaudio </p>
        <p> <a href="examples/javascript/oscilloscope_and_spectroscope.html"><img src="examples/javascript/oscilloscope_and_spectroscope.png" alt="Example 2js" border=0></a> </p>
        <!--
        <div id="video-script2" class="row">
          <div id="video2" class="col">
            <video controls
                   src="examples/javascript/oscilloscope_and_spectroscope_tutorial_pt1.mp4"
                   width="100%"
                   height="100%"
                   >
          </div>
          <div id="video3" class="col">
            <video controls
                   src="examples/javascript/oscilloscope_and_spectroscope_tutorial_pt2.mp4"
                   width="100%"
                   height="100%"
                   >
          </div>
        </div>
        <div id="video-script3" class="row">
          <div id="video4" class="col">
            <video controls
                   src="examples/javascript/oscilloscope_and_spectroscope_tutorial_pt3.mp4"
                   width="100%"
                   height="100%"
                   >
          </div>
          <div id="video5" class="col">
            <video controls
                   src="examples/javascript/oscilloscope_and_spectroscope_tutorial_pt4.mp4"
                   width="100%"
                   height="100%"
                   >
          </div>
        </div>
        <p>(Download the <a href="examples/javascript/oscilloscope_and_spectroscope.html">html file</a> and the <a href="examples/javascript/oscillator_oscilloscope_spectroscope.js">js file</a>)</p>
      -->
        <p>Try this <a href="examples/javascript/oscilloscope_and_spectroscope.html">html file</a> and check the <a href="examples/javascript/oscillator_oscilloscope_spectroscope.js">js file</a> too.</p>
        <ul>
          <li><a href="topics/intro-to-web-audio-api/3.oscilloscope_and_spectroscope-lesson.html">See lesson on Oscilloscope and Spectroscope in Webaudio</a></li>
        </ul>
        <hr>
        <p> <a name="Ex03js"><b>Example 3</b></a>: 3 ways to get your own Local Web Server </p>
        <!--
        <div id="video-script4" class="row">
          <div id="video6" class="col">
            <video controls
                   src="examples/javascript/local_web_server_tutorial.mp4"
                   width="100%"
                   height="100%"
                   >
          </div>
          <div id="script2" class="col">
            <button id="question-2" type="button" class="btn btn-primary question">Show/Hide Text</button>
            <div id="answer-2" style="display: none; overflow-y: scroll">

              <h3>3 ways to get your own Local Web Server</h3>

              <p>
                This is a video on 3 ways to get a Local Web Server running. Method 1: using Nodejs. Navigate to nodejs.org and install Nodejs. Once you have Nodejs installed, navigate to npmjs.com and search for "local web server". Open the package page and scroll down to copy the install command. Open a terminal window and paste the install code and run it. Once you have both Nodejs and Local Web Server installed, navigate to the folder for which you want to serve its files. Type "ws" and press enter. You web server is running on port 8000. On a browser, navigate to localhost:8000. These are the files that you are serving. Open an html page, and confirm that it is working. Go back to the terminal window, and press control-C to end your server. Method 2: using Python. If you have Python 2.7 installed on your machine, you can navigate to the folder that you want to host and type: python -m SimpleHTTPServer. Then press enter and your server will be running on port 8000. On a browser, navigate to localhost:8000. Open a file, and confirm that it is working. Go back to the terminal window and push control-C to end your server. Method 3: using google chrome. Open Google and search for: google chrome web server. Open the link for the chrome web store and install Web Server for Chrome. After installing Web Server for Chrome, open a new tab and navigate to apps to open the Web Server. On the window that opens, you might want to click "Choose Folder" to select the folder that you want to host. Copy the web server URL, and paste it in the browser. Open a file, and confirm that it is working. Go back to the Web Server for Chrome window to disable the webserver.
              </p>
            </div>
          </div>
        </div>
      -->
        <p> These are 3 ways to get a Local Web Server running. Method 1: using Nodejs. Navigate to nodejs.org and install Nodejs. Once you have Nodejs installed, navigate to npmjs.com and search for "local web server". Open the package page and scroll down to copy the install command. Open a terminal window and paste the install code and run it. Once you have both Nodejs and Local Web Server installed, navigate to the folder for which you want to serve its files. Type "ws" and press enter. You web server is running on port 8000. On a browser, navigate to localhost:8000. These are the files that you are serving. Open an html page, and confirm that it is working. Go back to the terminal window, and press control-C to end your server. Method 2: using Python. If you have Python 2.7 installed on your machine, you can navigate to the folder that you want to host and type: python -m SimpleHTTPServer. Then press enter and your server will be running on port 8000. On a browser, navigate to localhost:8000. Open a file, and confirm that it is working. Go back to the terminal window and push control-C to end your server. Method 3: using google chrome. Open Google and search for: google chrome web server. Open the link for the chrome web store and install Web Server for Chrome. After installing Web Server for Chrome, open a new tab and navigate to apps to open the Web Server. On the window that opens, you might want to click "Choose Folder" to select the folder that you want to host. Copy the web server URL, and paste it in the browser. Open a file, and confirm that it is working. Go back to the Web Server for Chrome window to disable the webserver. </p>
        <ul>
          <li><a href="topics/intro-to-web-audio-api/4.3_ways_to_local_web_server-lesson.html">See lesson on 3 ways to get your own Local Web Server</a></li>
        </ul>
        <hr>
        <p>
          <a name="Ex04js"><b>Example 4</b></a>: Volume Fader
        </p>
        <p>
          This example demonstrates a way to give a user control over the volume of a sound. It also demonstrates a way to do a timed fade-in or fade-out, using the <span style="font-family: monospace">setInterval()</span> method in JavaScript.
        </p>
        <p>
          <a href="examples/javascript/volumefaderjs.html">Volume Fader &lt;volumefaderjs.html&gt;</a>
        </p>
        <hr>
        <p>
          <a name="Ex05js"><b>Example 5</b></a>: Volume Fader in Decibels
        </p>
        <p>
          To make the change in a slider correspond well to our subjective sense of loudness, in JavaScript we need to map the slider value to the 0-to-1 range of volume using an exponential mapping. This example demonstrates a way to give a user control over the volume of a sound, with a linear change in the slider position actually causing an exponential change in the amplitude of the sound playback. It also demonstrates a way to do a timed fade-in or fade-out, using the <span style="font-family: monospace">setInterval()</span> method in JavaScript. You can compare and contrast this effect with the linear fade demonstrated in <a href="#Ex04js">Example 4</a>.
        </p>
        <p>
          <a href="examples/javascript/volumefaderdbjs.html">Volume Fader in Decibels &lt;volumefaderdbjs.html&gt;</a>
        </p>
        <hr>
        <p>
          <a name="Ex06js"><b>Example 6</b></a>: Show MIDI Input
        </p>
        <p>
          This example shows how to get MIDI input in a browser, how to parse MIDI channel messages, and how to access and interpret the data bytes. It's just a MIDI "monitor" that looks at the incoming MIDI messages and reports the information. This could serve as the foundation for an application that uses MIDI messages to control a synthesizer.
        </p>
        <p>
          First, the page's JavaScript queries the browser to see if the browser understands the Web MIDI API. Just about all modern browsers do. If so, it then requests access to the available MIDI devices, requests a listing of the input devices, and assigns a handler function to the
        <span style="font-family: monospace">onmidimessage</span> event that might come from any of those input devices. The handler function will parse the MIDI message and respond to it.
        </p>
        <p>
          In this example, we print what kind of message it is, and if it's a MIDI channel message (status bytes 128-239) we report details of its contents. The first byte in a message is the status byte, of which bits 0-3 indicate the MIDI channel (a value 0-15 indicating MIDI channel 1-16) and bits 4-6 indicate the message type (note-on, note-off, polyphonic key pressure, continuous controller, program change, aftertouch, or pitchbend). We use bit-masking to isolate the bits of the status byte that we want to examine. The type of message determines how many additional data bytes there will be in the message, and what they mean.
        </p>
        <p>
          The information is printed into a scrolling text box, which is created by a <span style="font-family: monospace">&lt;div&gt;</span> element with <span style="font-family: monospace">style="overflow: auto"</span>. After each new line of text (each new MIDI message report) is added to that text box, we reset the scrolling position of the textbox to the bottom of the box, so that when the textbox fills to overflowing and begins to scroll, the most recently added message will be shown.
        </p>
        <p>
          <a href="examples/javascript/showmidijs01.html">Show MIDI Input &lt;showmidijs01.html&gt;</a>
        </p>
        <hr>
        <p>
          <a name="Ex07js"><b>Example 7</b></a>: Draw a Sound
        </p>
        <p>
          Once we get audio data into an array (an <span style="font-family: monospace">AudioBuffer</span>), we can graph the peak amplitudes in that data to show the sound's amplitude envelope. To get the sound into an array, we use an <span style="font-family: monospace">XMLHttpRequest</span> to get an audio file, put it in an array of binary data, decode it, and place it in the buffer of an <span style="font-family: monospace">AudioBufferSourceNode</span>. The <span style="font-family: monospace">&lt;canvas&gt;</span> element in the Web page provides a drawing space in which we can draw our graph with primitive drawing commands.
        </p>
        <p>
          It probably goes without saying that in order to draw a sound, which consists of tens of thousands of samples, in the space of a few hundred pixels, we'll need to reduce the amount of information substantially. To figure out how to do that, we divide the sound into "chunks"; we divide the total number of samples in the sound by the number of pixels there are in the horizontal (width) dimension of the canvas. We then step through each of those chunks, and in each such chunk of audio samples we look for the minimum and maximum value, and we draw a line to those values in the graph. The result is an overall depiction of the peaks and troughs in the signal, which shows us the amplitude envelope.
        </p>
        <p>
          <a href="examples/javascript/drawasoundjs.html">Draw a Sound &lt;drawasoundjs.html&gt;</a>
        </p>

        <hr/>

        <p>
          <a name="Ex08js"><b>Example 8</b></a>: Recurring timed events in HTML5
        </p>

        <P>
        Most programming languages provide a means of getting a numerical representation of the current time with millisecond or microsecond (or even nanosecond) accuracy, such as the System.nanoTime() method in Java and the <B>cpuclock</B> object in Max. By comparing one instant to another, you can measure time intervals or durations with great accuracy.
        </P>

        <P>
        In JavaScript in HTML5, the performance.now() method will report the current time with microsecond accuracy. Even without knowing the current time, though, you can schedule events to happen at specific moments in the future in JavaScript with (approximately) millisecond accuracy. The WindowTimers method setTimeout() lets you schedule a named function to be executed a certain number of milliseconds in the future. The format is <I>var theID = setTimeout( theFunction, N );</I> where N is the number of milliseconds in the future that you want theFunction() to be executed. You can also use a WindowTimers method called setInterval() to schedule a recurring function call every N milliseconds. The methods setTimeout() and setInterval() return a unique ID which can serve as pointer to that scheduled task. It's important to keep track of that pointer, by storing it in a variable, so that you can later clear (unschedule) the task with the method clearTimeout( theID ) or clearInterval( theID ).
        </P>

        <P>
        This Web page does nothing more than test the accuracy of the setInterval() method. The script uses the performance.now() method to measure how much time elapsed between the time setInterval() was called and the time it reports with each function call. Because performance.now() provides time with microsecond precision, we can see that each setInterval() function call might be off by some fraction of a millisecond. In fact, depending on how busy the client computer is, the timing could be off by more than that. However, this test appears to demonstrate that a) setInterval() seems to compensate for its own lateness on the next function call, so that it doesn't suffer from excessive time lag over the long term, and b) the inaccuracies are generally not so great as to be musically bothersome. The inaccuracies could be problematic in certain very time-crucial circumstances, though, and these WindowTimers methods are not sample-accurate for audio processing purposes, nor are they in any way explicitly related to the audio sample-rate timing of the Web Audio API.
        </P>

        <P>
        <A HREF="examples/javascript/testperformancenow.html">Test performance of the setInterval() method in HTML5</A>
        </P>

        <hr/>

        <p>
          <a name="Ex09js"><b>Example 9</b></a>: Scheduling timed notes in HTML5
        </p>

        <P>
          Although the WindowTimers.setInterval() method is not explicitly linked to the sample-rate timing of the Web Audio API, in many cases it can be sufficiently accurate for timing of audio events. In this example we use setInterval() to schedule a recurring pattern of notes events in Web Audio API. Try it and see if it is adequately accurate on your computer.
        </P>

        <P>
        The script creates an array of sixteen pitches and an array of sixteen loudnesses, and uses those arrays to play a recurring pattern of sixteen synthesized notes in a loop, with each note scheduled by setInterval(). Note that the duration of each note is not quite as long as the time interval between note onsets. That produces a very slightly <I>staccato</I> or <I>d&eacute;tach&eacute;</I> articulation of the notes, but more importantly it avoids any potentially disparity between the timing of the notes' amplitude envelope and the timing of the setInterval() method.
        </P>

        <P>
        This script also includes handy functions mtof() for converting MIDI-style pitch numbers to their corresponding frequency, and dbtoa() for converting loudness in decibels into their corresponding linear amplitude.
        </P>

        <P>
          <A HREF="examples/javascript/intervalscheduling.html">Schedule notes with setInterval()</A>
        </P>

        <hr/>

        <p>
          <a name="Ex10js"><b>Example 10</b></a>: Scheduling audio events in Web Audio API
        </p>

        <P>
        In the Web Audio API a parameter of an audio node (an AudioParam) can be changed with sample-accurate timing. Each AudioNode object, such as an oscillator node or a gain node, has one or more properties&mdash;the frequency value of an oscillator node or the gain value of a gain node&mdash;which can be set at specific times, either immediately or in the future.
        </P>

        <P>
        In this example, we create our own "instrument" object consisting of an oscillator node and a gain node, and then we schedule notes to be played on that instrument at specific times by setting the frequency of the oscillator node and the gain of the gain node. To do that, we create an array of pitches and an array of volumes, and establish a beat tempo. Then we step through the two arrays, scheduling different pitches and volumes to happen at specific times based on the tempo. The program schedules 64 notes at once, then plays them.
        </P>

        <P>
          <A HREF="examples/javascript/play64notes.html">Schedule and play 64 notes</A>
        </P>

        <hr/>

        <p>
          <a name="Ex11js"><b>Example 11</b></a>: Delay with Feedback in Web Audio API
        </p>

        <p>
          To create echo effects, one needs to create a <a href="https://dobrian.github.io/cmp/topics/delay-based-effects/circularbuffer.html">circular buffer</a> to store and recall recent sound data. The Web Audio API provides an AudioNode interface called DelayNode that implements this digital delay line for you.
        </p>

        <p>
          When you create the DelayNode, you specify a maxDelayTime (or, if not specified, it is 1 second by default), and that sets aside enough memory to store the corresponding number of samples. The <i>delayTime</i> property specifies how far in the past the DelayNode will look in its input buffer to determine what to send out as its output. Thus, the DelayNode in Web Audio API is comparable to <a href="https://music.arts.uci.edu/dobrian/maxcookbook/simple-delay-audio-signal">the <b>delay~</b> object in MSP</a>.
        </p>

        <p>
          Sending the output of a digital delay line back into its input presents the potential danger of infitine recursion (calculating the delay of a delay of a delay, <i>et cetera ad infinitum</i>), so if you want a feedback delay, the <i>delayTime</i> must always be at least as great as the block of samples being calculated by the system. In MSP, the <b>delay~</b> object does not allow feedback, and to do feedback you must <a href="https://music.arts.uci.edu/dobrian/maxcookbook/delay-feedback">use the <b>tapin~</b> and <b>tapout~</b> objects</a> instead, which have a minimum delay time of one signal vector (the block of samples that MSP objects calculate at one time). In Web Audio API, you can create a feedback loop in which the output of a DelayNode is eventually routed back into its input, but in that case the minimum delay time will automatically be set at one <i><a href="https://webaudio.github.io/web-audio-api/#render-quantum">render quantum</a></i>, which is 128 samples (thus, 2.75 ms at a sampling rate of 48 KHz).
        </p>

        <p>
          Whenever you implement feedback in an audio delay line, you need to diminish the amplitude of the signal being fed back, otherwise the sum of the feedback and the input signal will accumulate and grow out of control. In this example, we include volume control at three points in the system (with three Gain nodes)&mdash;gain of the sound source being used as input to the Delay node, gain of of the output of the Delay node, and gain of that output signal being used as feedback into the Delay node&mdash;and we give the user control over each of those gains.
        </p>

        <p>
          <img src="images/delaywithfeedback01.png" height="509" width="424" border="0">
        </p>

        <P>
          <A HREF="examples/javascript/delaywithfeedback01.html">Delay With Feedback in Web Audio API</A>
        </P>

        <hr/>

        <p>
          <a name="Ex12js"><b>Example 12</b></a>: Stereo panning in Web Audio API
        </p>

        <P>
          In the Web Audio API there is a <a href="https://developer.mozilla.org/en-US/docs/Web/API/PannerNode">PannerNode</a> for altering the intensity of the sound in each of the two stereo channels according to its onstensible location and orientation in virtual 3D space. The PannerNode allows you to take into account the sound source's virtual location (X, Y, and Z position) relative to the listener's position, the sound source's orientation (which direction it's facing), the directionality of the sound source (how much the intensity diminishes when the source is not facing right at the listener), and various other settings for how much the intensity should change based on those factors. The PannerNode deals only with the amplitude of the sound, and doesn't take into account other possible alterations such as filtering, reverberation, etc. However, it does offer an "<a href="https://webaudio.github.io/web-audio-api/#Spatialization-hrtf-panning">HRTF</a>" panning mode that uses <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">head-related transfer functions</a>.
        </P>

        <P>
          Web Audio API also provides a simplified <a href="https://developer.mozilla.org/en-US/docs/Web/API/StereoPannerNode">StereoPannerNode</a>, with just one parameter, <a href="https://developer.mozilla.org/en-US/docs/Web/API/StereoPannerNode/pan"><i>pan</i></a>, that lets you specify the sound source's virtual location on the X axis. The StereoPannerNode implements amplitude-based stereo panning employing <a href="https://webaudio.github.io/web-audio-api/#stereopanner-algorithm">an efficient constant-intensity algorithm</a> that's essentially identical to the method shown in the Max Cookbook examples "<a href="https://music.arts.uci.edu/dobrian/maxcookbook/constant-power-panning-using-table-lookup">Constant power panning using table lookup</a>" and "<a href="https://music.arts.uci.edu/dobrian/maxcookbook/constant-intensity-panning-subpatch">Constant-intensity panning subpatch</a>". That's the method shown in this example. (The MDN Web Audio API reference page for StereoPannerNode also provides a simple and clear <a href="https://mdn.github.io/webaudio-examples/stereo-panner-node/">StereoPannerNode example</a>.)
        </P>

        <P>
          In this example, we create our own "instrument" consisting of a 10-millisecond burst of white noise that goes to a StereoPannerNode and then to a master volume GainNode. The Start button uses the setInterval() method to set up a repeating callback to a playNote() function that periodically (5 times per second) opens a 10 ms window on the white noise. You can use the sliders to control volume and panning, and you can also use the Pan Left and Pan Right buttons to do a timed pan to one speaker or the other.
        </P>

        <P>
          <A HREF="examples/javascript/panningdemojs.html">Stereo Panning Demo &lt;panningdemojs.html&gt;</A>
        </P>

        <hr/>
      </div>
    </section>
  </div>
  <footer class="secondary_header footer">
    <p class="footer">This page was last modified <strong>May 24, 2020</strong>.</p>
  </footer>
</div>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
    <script>
    //$('pre').addClass("javascript");
    hljs.initHighlightingOnLoad();
    </script>-->
<script>
      var videoHeight2 = $("#video6 > video").height();
      $("#answer-2").height(videoHeight2 - 50);
    </script>
<script src="/js/cmp.js"></script>
</body>
</html>
